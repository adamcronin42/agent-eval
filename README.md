# ğŸ¤– Agent-Eval

## âœ¨ What is Agent-Eval?

Agent-Eval is a **Python framework for building LLM agents with tools**. Go from empty folder to a working agent with custom tools in minutes.

### ğŸš€ **What You Get**
- âš¡ **Quick setup** - `pip install` to working agent in minutes
- ğŸ”§ **Simple tools** - Drop a Python class in `/tools`, it's automatically discovered
- ğŸŒ **Any model** - Gemini, OpenAI, Claude, or 100+ others via LiteLLM
- ğŸ“Š **Built-in metrics** - Token usage, timing, and conversation tracking
- ğŸ“œ **File-based prompts** - Version control your system prompts
- ğŸ¨ **Beautiful CLI** - Interactive mode with colors and helpful commands

### ğŸ¯ **Perfect For**
- **Rapid prototyping** - Test agent ideas quickly
- **Production development** - Scale from prototype to production
- **Multi-model experiments** - Easy switching between LLM providers
- **Team collaboration** - Git-friendly structure for shared development

---

## âš¡ Quick Start - Be Running in 60 Seconds

```bash
# 1. Install
pip install -e .

# 2. Set your API key (Gemini has a generous free tier, no credit card required!)
export GEMINI_API_KEY="your_key_here"

# 3. Chat with your agent instantly
agent-eval run "What are some great movies that came out this year?"

# 4. Start interactive mode
agent-eval chat
```

**That's it!** ğŸ‰ You now have a fully functional LLM agent with tool support, conversation management, and evaluation metrics.

---

## ğŸ”§ Tool Development - The Magic Happens Here

Creating tools is **ridiculously simple**. Just implement the `Tool` class anywhere in the `/tools` directory and the agent will automatically discover and use it:

```python
from agent_eval.tools import Tool

class WeatherTool(Tool):
    def get_schema(self):
        return {
            "name": "get_weather",
            "description": "Get current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "City name"}
                },
                "required": ["location"]
            }
        }

    def execute(self, location: str):
        # Your implementation here
        return f"Weather in {location}: Sunny, 72Â°F"
```

**That's literally it!** ğŸ¤¯

- âœ… No registration required
- âœ… Auto-discovery at runtime
- âœ… Automatic validation
- âœ… Built-in error handling
- âœ… Tool approval workflow

---

## ğŸ“œ System Prompt Management

Agent-Eval treats prompts as **first-class citizens** with proper version control and organization:

```
your-project/
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ default_system.txt        # Main system prompt
â”‚   â”œâ”€â”€ customer_support.txt      # Domain-specific variants
â”‚   â”œâ”€â”€ data_analyst.txt          # Role-specific prompts
â”‚   â””â”€â”€ llm_judge_evaluation.txt  # Custom evaluation criteria
```

**Features:**
- ğŸ¯ **File-based prompts** - Easy to version control and collaborate
- ğŸ”„ **Hot-swapping** - Test different prompts instantly
- ğŸ“Š **A/B testing** - Compare prompt performance systematically
- ğŸ¨ **Template system** - Reusable prompt components

```bash
# Test different prompts instantly
agent-eval chat --system-prompt="prompts/customer_support.txt"

# Hot-swap during development
echo "You are a specialized data analyst..." > prompts/custom.txt
agent-eval chat --system-prompt="prompts/custom.txt"
```

---

## ğŸŒ Any LLM Provider - Your Choice

Agent-Eval supports **100+ models** through LiteLLM integration:

### ğŸ†“ **Start Free with Gemini**
```bash
export GEMINI_API_KEY="your_key"
# Generous free tier: 15 RPM, 1M tokens/day
```

### ğŸš€ **Scale with Premium Providers**
```bash
# OpenAI
export OPENAI_API_KEY="your_key"
agent-eval chat --model="openai/gpt-4"

# Anthropic Claude
export ANTHROPIC_API_KEY="your_key"
agent-eval chat --model="anthropic/claude-3-sonnet-20240229"

# Or any other provider...
```

**Supported Providers:**
- **Gemini** (Free tier available!)
- **OpenAI** (GPT-4, GPT-3.5)
- **Anthropic** (Claude family)
- **Cohere, Hugging Face, Azure** and 100+ more

---

## ğŸ¨ Beautiful CLI Experience

```bash
# Interactive mode with colors and emojis
$ agent-eval chat

ğŸš€ Initializing agent...

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           ğŸ¤– Agent-Eval              â•‘
â•‘    LLM Agent Evaluation Framework    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Model: gemini/gemini-2.5-flash
Tools: ask_user, search_web
Auto-approve: No

ğŸ’¬ Ready! Type '/help' for commands

You: What's the capital of France?
ğŸ¤– Agent: The capital of France is Paris.

ğŸ“Š Tokens: 245 | Tools: 0 | Iterations: 1 | Time: 0.8s
```

### ğŸ› ï¸ Built-in Commands
- `/help` - Show available commands
- `/tools` - List discovered tools
- `/metrics` - Show performance stats
- `/save` - Save conversation to file
- `/clear` - Reset conversation

---

## ğŸ“Š Built-in Metrics & Tracking

Every interaction is automatically tracked and monitored:

### ğŸ” **Real-time Metrics**
- Token usage per query
- Tool execution count
- Response time tracking
- Error rate monitoring
- Iteration counting

### ğŸ’¾ **Conversation Persistence**
```json
// Auto-saved to conversation_history/
{
  "conversation_history": [...],
  "metrics": {
    "total_tokens": 1250,
    "tool_calls": 3,
    "iterations": 2,
    "duration": 2.34
  },
  "model": "gemini/gemini-2.5-flash",
  "tools_available": ["search_web", "ask_user"]
}
```

---

## ğŸ”® What's Coming Next

### ğŸ”„ The Agent Development Loop *(The Secret to Production-Ready Agents)*

The professional workflow that scales from prototype to bulletproof agents:

```
ğŸ“ Create Test Cases â†’ ğŸš€ Run Evaluation â†’ ğŸ“Š Review Results
     â¬†                                           â¬‡
ğŸ” Add More Tests  â† âœ… Passes? â†’ ğŸ¯ Tweak System Prompt
```

**The Flow:**
1. **ğŸ“ Write test cases** in `golden_dataset/`
2. **ğŸš€ Run evaluation**: `agent-eval evaluate --dataset golden_dataset/`
3. **ğŸ“Š Analyze failures** in evaluation reports
4. **ğŸ¯ Refine system prompt** in `prompts/default_system.txt`
5. **ğŸ” Re-evaluate** until tests pass
6. **â• Add edge cases** you discovered
7. **ğŸ”„ Repeat** â†’ Build bulletproof agents

This iterative workflow is what separates toy demos from production-grade agents.

### ğŸ“Š Evaluation Pipeline *(Coming Soon)*

The missing piece for production agent evaluation:

```
your-project/
â”œâ”€â”€ golden_dataset/           # Your test cases
â”‚   â”œâ”€â”€ customer_support.json
â”‚   â””â”€â”€ data_analysis.json
â”œâ”€â”€ evaluation_logs/          # Automated runs
â”‚   â””â”€â”€ run_2025_01_15_14_30/
â”‚       â”œâ”€â”€ responses.json
â”‚       â”œâ”€â”€ metrics.json
â”‚       â””â”€â”€ failures.log
â””â”€â”€ llm_judge_reports/        # AI evaluation
    â”œâ”€â”€ accuracy_scores.json
    â””â”€â”€ quality_analysis.md
```

**How it will work:**

1. **ğŸ“ Define Test Cases** - JSON files with input/expected output pairs
2. **ğŸš€ Run Evaluation** - `agent-eval evaluate --dataset golden_dataset/`
3. **ğŸ¤– LLM-as-Judge** - Automatic quality scoring for subjective responses
4. **ğŸ“ˆ Detailed Reports** - Pass/fail rates, performance trends, insights
5. **ğŸ”„ CI/CD Integration** - Automated testing in your deployment pipeline

**Enhanced Evaluation Features:**

ğŸ¯ **Custom LLM-as-Judge Prompts**
```
prompts/
â”œâ”€â”€ llm_judge_evaluation.txt    # Your custom evaluation criteria
â”œâ”€â”€ accuracy_rubric.txt         # Domain-specific scoring
â””â”€â”€ safety_evaluation.txt       # Safety/compliance checks
```

ğŸ“Š **Evaluation Dashboard**
- Pass/fail rates by test case category
- Prompt performance comparisons
- Regression detection across iterations
- Cost analysis per evaluation run

ğŸ”„ **CI/CD Integration**
```bash
# In your GitHub Actions
- name: Evaluate Agent
  run: agent-eval evaluate --dataset golden_dataset/ --fail-threshold 0.95
```

This will make Agent-Eval the **complete solution** for agent development and evaluation.

---

## ğŸš€ Advanced Usage

### ğŸ›ï¸ **Custom Configuration**
```bash
# Override any setting
agent-eval chat \
  --model="openai/gpt-4" \
  --max-iterations=15 \
  --auto-approve \
  --system-prompt="You are a helpful coding assistant"
```


---

## ğŸ—ï¸ Installation & Setup

### ğŸ“¦ **Quick Install**
```bash
pip install -e .
```

### ğŸ **Development Setup**
```bash
# Clone and setup
git clone <repository-url>
cd agent-eval
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -e .

# Run tests
python test_agent.py  # Comprehensive agent tests
python -m pytest tests/ -v  # Tool-specific tests
```

### ğŸ”‘ **API Key Setup**
```bash
# Option 1: Environment variables
export GEMINI_API_KEY="your_key"
export OPENAI_API_KEY="your_key"

# Option 2: .env file
echo "GEMINI_API_KEY=your_key" > .env
```

---

## ğŸ›ï¸ Architecture

Agent-Eval is built on solid foundations:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Beautiful CLI â”‚â”€â”€â”€â”€â”‚  Agent Core      â”‚â”€â”€â”€â”€â”‚  Tool Discovery â”‚
â”‚  â€¢ Colored UI   â”‚    â”‚  â€¢ Conversation  â”‚    â”‚  â€¢ Auto-detect  â”‚
â”‚  â€¢ Interactive  â”‚    â”‚  â€¢ Metrics       â”‚    â”‚  â€¢ Validation   â”‚
â”‚  â€¢ Intuitive    â”‚    â”‚  â€¢ Prompt Mgmt   â”‚    â”‚  â€¢ Execution    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚                         â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
                    â”‚   Prompt System     â”‚             â”‚
                    â”‚  â€¢ File-based       â”‚             â”‚
                    â”‚  â€¢ Version Control  â”‚             â”‚
                    â”‚  â€¢ Hot-swapping     â”‚             â”‚
                    â”‚  â€¢ A/B Testing      â”‚             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                               â”‚                         â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                       â”‚   LiteLLM      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚  â€¢ 100+ Models â”‚
                       â”‚  â€¢ Unified API â”‚
                       â”‚  â€¢ Reliability â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Core Components:**
- **ğŸ¨ CLI Layer** - Beautiful, intuitive developer experience
- **ğŸ§  Agent Core** - Conversation management, metrics, error handling
- **ğŸ“œ Prompt System** - File-based prompts with version control and A/B testing
- **ğŸ”§ Tool System** - Zero-config auto-discovery and execution
- **ğŸŒ LiteLLM Integration** - Universal model provider support

---

## ğŸ› **Found a Bug?**
[Open an issue](https://github.com/your-org/agent-eval/issues) with:
- Clear description
- Steps to reproduce
- Expected vs actual behavior
- Your environment details


---

## ğŸŒŸ Star Us!

If Agent-Eval makes your LLM agent development easier, **please star the repo**! â­

Every star helps us reach more developers who are struggling with complex agent frameworks.

---

<div align="center">

**Built with â¤ï¸ for the LLM developer community**

</div>